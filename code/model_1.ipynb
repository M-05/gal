{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6a6f22",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379e5ee",
   "metadata": {},
   "source": [
    "- 목적 : input data 를 not, exp로 분류하고 일정 threshold 이하일 때에는 ood로 판단하는 모델을 학습시킨 후 model_1로 저장한다.    \n",
    "- 학습 데이터 : 일상대화문, 비윤리대화문 중 intensity '2.0' 이상을 추출한 데이터중에 VIOLENCE라벨을 가지고있는 것들, dktc_exp\n",
    "- 참고사항 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcfa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-bert\n",
    "!pip install keras-radam\n",
    "!pip install wandb\n",
    "\n",
    "# !pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bf5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import 및 주요 라이브러리 버전 확인 \n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras_radam.training import RAdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoding_korbert as enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7eb0d2",
   "metadata": {},
   "source": [
    "## 학습에 쓸 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b16bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_train_df = pd.read_csv(\"./data/daily_train.csv\", usecols=['class','conversation']) #일상대화 train 병합 데이터\n",
    "daily_val_df = pd.read_csv(\"./data/daily_val.csv\", usecols=['class','conversation']) #일상대화 val 병합 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dktc_exp_df = pd.read_csv(\"./data/dktc_exp.csv\", usecols=['class','conversation'])\n",
    "immoral_df = pd.read_csv(\"./data/immoral_over_2_v.csv\", usecols=['class','conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c6206e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_exp_df = pd.concat([dktc_exp_df, immoral_df], axis=0)\n",
    "merged_df = merged_exp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e70616",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_exp_train_df, model_1_exp_val_df = train_test_split(merged_exp_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a26589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 샘플링 코드\n",
    "\n",
    "# daily_train_df =daily_train_df.sample(n=50, random_state=1004)\n",
    "# daily_val_df =daily_val_df.sample(n=10, random_state=1004)\n",
    "\n",
    "# model_1_exp_train_df =model_1_exp_train_df.sample(n=50, random_state=1004)\n",
    "# model_1_exp_val_df =model_1_exp_val_df.sample(n=10, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83ed3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_train_df = pd.concat([daily_train_df, model_1_exp_train_df], join=\"inner\")\n",
    "model_1_train_df.reset_index(drop=\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59d50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_val_df = pd.concat([daily_val_df, model_1_exp_val_df], join=\"inner\")\n",
    "model_1_val_df.reset_index(drop=\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa39114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model_1_train_df = model_1_train_df.sample(frac=1).reset_index(drop=True) #많이 섞기\n",
    "\n",
    "for i in range(10):\n",
    "    model_1_val_df = model_1_val_df.sample(frac=1).reset_index(drop=True)#많이 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4199e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_train_df[\"class\"] = model_1_train_df[\"class\"].astype('category')\n",
    "model_1_train_df[\"class\"] = model_1_train_df[\"class\"].cat.codes # 0:exp, 1:not\n",
    "\n",
    "model_1_val_df[\"class\"] = model_1_val_df[\"class\"].astype('category')\n",
    "model_1_val_df[\"class\"] = model_1_val_df[\"class\"].cat.codes # 0:exp, 1:not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7fc17",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11548c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = enc.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43753b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = enc.load_data(model_1_train_df)\n",
    "val_x, val_y = enc.load_data(model_1_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e72eb",
   "metadata": {},
   "source": [
    "### 모델 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada12c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"fff4dcdf86063b9dafa0296b4abaeb7d3639da7d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd32b52",
   "metadata": {},
   "source": [
    "### init_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "#     \"n_channel_1\" : 32,\n",
    "#     \"n_channel_2\" : 64,\n",
    "#     \"n_dense\" : 1024,\n",
    "    \"learning_rate\" : 1e-4,\n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 16,\n",
    "    \"weight_decay\" : 0.025,\n",
    "    \"optimizer\" : \"radam\",\n",
    "    \"loss\" : \"binary_crossentropy\",\n",
    "    \"metrics\" : [\"accuracy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad62957",
   "metadata": {},
   "source": [
    "### sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"optimizer\" : {\n",
    "            'value' : 'radam'\n",
    "            },\n",
    "        \"batch_size\" : {\n",
    "            \"values\" : [8, 16] # OOM 에러시 바꿔주세용\n",
    "            },\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 1e-5,\n",
    "            \"max\" : 1e-4 # 0에 가까울 수록\n",
    "            },\n",
    "        \"weight_decay\" : {\n",
    "            \"values\" : [0.025, 0.001]\n",
    "        },\n",
    "        \"epochs\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 3,\n",
    "            \"max\" : 5\n",
    "            }\n",
    "                    \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4d4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path =\"./bert\" #상대경로 잡기\n",
    "\n",
    "SEQ_LEN = enc.SEQ_LEN\n",
    "\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8814c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = load_trained_model_from_checkpoint(config_path,\n",
    "                                                checkpoint_path,\n",
    "                                                training=True,\n",
    "                                                trainable=True,\n",
    "                                                seq_len=SEQ_LEN)\n",
    "    \n",
    "    inputs = model.inputs[:2]\n",
    "    dense = model.layers[-3].output\n",
    "    \n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid', \n",
    "                               kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                              name = 'real_output')(dense)\n",
    "\n",
    "    bert_model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78449c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_optimizer():\n",
    "#     if config.optimizer == \"radam\":\n",
    "#         return RAdamOptimizer(learning_rate=config.learning_rate, \n",
    "#                               weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandbCallback's labels\n",
    "CLASS_NAMES = [\"위협 대화\", \"일상 대화\"] # 0:exp, 1:not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= './saved'\n",
    "\n",
    "layer_num = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_callbacks():                       # 날짜 바꿔주세용\n",
    "    CK = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(path, \"model_1_weights_0419.{epoch:02d}-{val_loss:.2f}.h5\"),\n",
    "                                              monitor='val_loss',\n",
    "                                              save_best_only = True,\n",
    "                                              save_weights_only = True)\n",
    "\n",
    "    ES = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "    \n",
    "    return CK, ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global default_config\n",
    "\n",
    "    # wandb.init & config ok\n",
    "    run = wandb.init(project = 'model_1_0419', # 날짜만 변경해주세요\n",
    "                    entity = \"m05\",\n",
    "                    config = default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Model ok\n",
    "    model = build_model()    \n",
    "    \n",
    "    # optimizer 함수 불러오기 ok\n",
    "#     optimizer = set_optimizer()\n",
    "    if config.optimizer == \"radam\":\n",
    "        OPTIMIZER = RAdamOptimizer(learning_rate=config.learning_rate, \n",
    "                              weight_decay=config.weight_decay)\n",
    "    \n",
    "    # model.compile ok\n",
    "    model.compile(optimizer = OPTIMIZER,\n",
    "                  loss = config.loss,\n",
    "                  metrics= config.metrics)\n",
    "    \n",
    "    #keras_callbacks ok\n",
    "    CK, ES = keras_callbacks()\n",
    "    \n",
    "    # model.fit\n",
    "    model.fit(train_x, train_y,\n",
    "              epochs = config.epochs,\n",
    "              batch_size = config.batch_size,\n",
    "              validation_data = (val_x, val_y),\n",
    "              callbacks = [CK, ES , \n",
    "                           WandbCallback(training_data = (train_x[:30], train_y[:30]),\n",
    "                                         validation_data = (val_x[:30], val_y[:30]),\n",
    "                                        labels = CLASS_NAMES)])\n",
    "                                        \n",
    "    # weight 모델 저장\n",
    "    tf.keras.models.save_model(model, 'model_1')\n",
    "    \n",
    "#     # evaluate test\n",
    "#     test_loss, test_accuracy = model.evaluate(test_x, test_y, verbose=2)\n",
    "    \n",
    "#     # wandb's 테이블쪽 column 추가하기\n",
    "#     wandb.log({\"Test Accuracy Rate: \" : round(test_accuracy * 100, 2),\n",
    "#                \"Test Error Rate: \" : round((1 - test_accuracy) * 100, 2)})\n",
    "    \n",
    "    run.finish() # run 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805d01a",
   "metadata": {},
   "source": [
    "### model.fit 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d99698",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = 'm05', \n",
    "                       project = 'model_1_0419') # 날짜만 변경해주세요\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id,\n",
    "            count=5, # 몇회 돌릴지 선택해주세요\n",
    "            function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e355a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e2f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a61cb376",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40186d64",
   "metadata": {},
   "source": [
    "### model summary 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85eb2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 256, 768), ( 91812096    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 256, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 256, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 256, 768)     196608      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 256, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 256, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 256, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 256, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 256, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "real_output (Dense)             (None, 1)            769         NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 177,657,601\n",
      "Trainable params: 177,657,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1= build_model()\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840bed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이어의 trainable 출력해서 확인하기\n",
    "\n",
    "for layer in model_1.layers:\n",
    "    if 'Encoder' in layer.name:\n",
    "        if 'layer_0' in layer.name:\n",
    "            if layer.trainable == False:\n",
    "                print(f'{layer.name} is frozen, 고정됨')\n",
    "            else:\n",
    "                print(f'{layer.name} is not frozen, 고정안됨')\n",
    "        elif layer.trainable == False:\n",
    "            print(f'{layer.name} is frozen, 고정됨')\n",
    "        else:\n",
    "            print(f'{layer.name} is not frozen, 고정안됨')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model_1.layers[:layer_num]:\n",
    "#     layer.trainable = False # 층을 선택하여 freeze 하기."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
