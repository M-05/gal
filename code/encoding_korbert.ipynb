{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gal data Encoding functions by korBERT encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-bert\n",
    "!pip install keras-radam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input을 txt로 받았을 때 데이터프레임을 만드는 함수\n",
    "def make_table(txt_path):\n",
    "    files = glob.glob(txt_path)  \n",
    "    \n",
    "    df =  pd.DataFrame(index=range(0,len(files)), columns=['idx', 'class','conversation'])\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        f = open(file, 'r', encoding=\"UTF-8\")\n",
    "        line = f.readlines()\n",
    "        df['idx'][i] = i\n",
    "        # df['class'][i] = #레이블달기 전이니까 빈칸\n",
    "        df['conversation'][i] = line\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#korBERT 사전으로 tokenize\n",
    "pretrained_path =\"./bert\" #상대경로 잡기\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "SEQ_LEN = 256\n",
    "DATA_COLUMN = \"conversation\"\n",
    "LABEL_COLUMN = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token 딕셔너리 만들기\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        if \"_\" in token:\n",
    "            token = token.replace(\"_\",\"\")\n",
    "            token = \"##\" + token\n",
    "        token_dict[token] = len(token_dict)\n",
    "        #key(문자) = value(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 클래스 만들기\n",
    "class inherit_Tokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = text\n",
    "            \n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
    "                spaced += ' ' + ch + ' '\n",
    "            elif self._is_space(ch):\n",
    "                spaced += ' '\n",
    "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = inherit_Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data_df): #korBERT의 토큰 딕셔너리로 데이터를 인코딩하는 함수\n",
    "    global tokenizer\n",
    "    indices, targets = [], []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN) # conversation\n",
    "        indices.append(ids)\n",
    "        targets.append(data_df[LABEL_COLUMN][i]) # class\n",
    "    items = list(zip(indices, targets))\n",
    "    \n",
    "    indices, targets = zip(*items)\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)], np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pandas_dataframe): # 데이터 인코딩을 실행하는 함수\n",
    "    data_df = pandas_dataframe\n",
    "    \n",
    "    \n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str) # conversation\n",
    "\n",
    "\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "\n",
    "    return data_x, data_y"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
